from typing import List, Dict, Union
import chromadb

from .vector_store import search_vector_store
from core.model_provider import llm

# Define Document type
Document = Dict[str, Union[str, Dict]]

class RAGRetriever:
    def __init__(self, collection: chromadb.Collection):
        """
        Initializes the RAG retriever.

        Args:
            collection (chromadb.Collection): The ChromaDB collection to retrieve documents from.
        """
        self.collection = collection
        self.llm = llm # Use the globally initialized LLM

    def _create_prompt(self, query: str, context_docs: List[Document]) -> str:
        """
        Creates a prompt based on the retrieved context and the user's query.
        """
        context = "\n\n".join([doc["page_content"] for doc in context_docs])
        
        prompt_template = f"""
        You are a helpful assistant. Answer the following question based only on the provided context.
        If the answer is not found in the context, say "I don't know".

        Context:
        ---
        {context}
        ---

        Question: {query}
        """
        return prompt_template.strip()

    def answer_query(self, query: str) -> str:
        """
        Executes the full RAG process: retrieve -> augment -> generate.

        Args:
            query (str): The user's query.

        Returns:
            str: The final answer generated by the LLM.
        """
        if not self.llm:
            return "Error: LLM is not available. Please check your API key."

        # 1. Retrieve
        print(f"Searching for context related to: '{query}'")
        retrieved_docs = search_vector_store(query, self.collection, n_results=3)
        
        if not retrieved_docs:
            print("No relevant context found in the vector store.")
            return "I could not find any relevant information to answer your question."

        print(f"Found {len(retrieved_docs)} relevant document chunks.")

        # 2. Augment
        prompt = self._create_prompt(query, retrieved_docs)
        print("\n--- Generated Prompt for LLM ---")
        print(prompt)
        print("---------------------------------")

        # 3. Generate
        print("\nGenerating answer from LLM...")
        response = self.llm.invoke(prompt)
        
        return response.content

# --- Example Usage ---
if __name__ == '__main__':
    # This example depends on the vector_store.py example running successfully.
    # We will recreate a temporary vector store here.
    from .vector_store import create_vector_store, client as chroma_client

    if not llm:
        print("Skipping retriever example because LLM is not available.")
    else:
        # 1. Prepare data and create vector store
        sample_chunks = [
            {"page_content": "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.", "metadata": {"source": "wiki/eiffel"}},
            {"page_content": "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.", "metadata": {"source": "wiki/eiffel"}},
            {"page_content": "The capital of France is Paris. It is known for its art, fashion, and culture.", "metadata": {"source": "wiki/paris"}}
        ]
        collection_name = "france_facts_openai"
        if collection_name in [c.name for c in chroma_client.list_collections()]:
            client.delete_collection(name=collection_name)
            
        collection = create_vector_store(sample_chunks, collection_name=collection_name)
        print(f"\nVector store '{collection.name}' created with {collection.count()} items.")

        # 2. Initialize RAGRetriever
        retriever = RAGRetriever(collection)

        # 3. Ask a question
        print("\n--- Answering a question ---")
        question = "Who designed the Eiffel Tower?"
        answer = retriever.answer_query(question)

        print(f"\nQuestion: {question}")
        print(f"Answer: {answer}")
        
        print("\n--- Answering another question ---")
        question_2 = "What is the capital of Spain?"
        answer_2 = retriever.answer_query(question_2)
        
        print(f"\nQuestion: {question_2}")
        print(f"Answer: {answer_2}")